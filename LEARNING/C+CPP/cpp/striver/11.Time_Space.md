### Time Complexity vs. Time Taken

**Time Complexity**

- **Definition**: Theoretical measure of the running time of an algorithm as a function of input size, expressed in Big O notation (e.g., \( O(n) \), \( O(n^2) \)).
- **Purpose**: Compare efficiency of algorithms independently of hardware or environment.
- **Example**: Searching in an unsorted list of \( n \) elements has a time complexity of \( O(n) \).

**Time Taken**

- **Definition**: Actual time taken by an algorithm to run on specific input and environment.
- **Purpose**: Measure real-world performance.
- **Example**: The same search algorithm might take 2 ms on one computer and 1 ms on a more powerful one.

**Key Differences**

- **Abstraction Level**: Time complexity is theoretical; time taken is practical.
- **Independence**: Time complexity is machine-independent; time taken is machine-dependent.
- **Purpose**: Time complexity compares algorithm efficiency; time taken measures real-world performance.
- **Predictive Power**: Time complexity predicts scalability; time taken provides immediate performance feedback.

## Big O Notation - O()

Big O notation describes the upper bound of the time complexity, representing the worst-case scenario of an algorithm.

### Example

```cpp
for(int i=1; i<=N; i++) {
    cout << "Hello";
}
```

In this example, the time complexity is O(N) because the loop runs `N` times.

## Best Case, Average Case, and Worst Case

Algorithms can have different time complexities based on the input.

- **Best Case:** O(1) - The algorithm performs the best.
- **Average Case:** O(N) - The average performance over a set of inputs.
- **Worst Case:** O(N) - The algorithm performs the worst.

### TC, worst-case scenario:

- **Avoid Constants:** They become insignificant as `N` grows.
- **Avoid lower values:** They have very little impact on the overall complexity.

## Big O, Theta, and Omega Notation

- **Big O (O):** Describes the upper bound or worst-case complexity.
- **Theta (Θ):** Describes the average-case complexity.
- **Omega (Ω):** Describes the lower bound or best-case complexity.

## Examples and Evaluation

### Example 1

```cpp
for(int i=0; i<N; i++) {
    for (int j=0; j<N; j++) {
        // Block of code
    }
}
```

**Evaluate:**
i=0 {j=0,1,2,...N}
i=1 {j=0,1,2,...N}
i=2 {j=0,1,2,...N}
.
.
i=N-1 {j=0,1,2,...N}
In the outer for loop, `i` iterates `N` times. The inner loop also iterates `N` times for each iteration of the outer loop, so the total complexity is `N * N = N^2`.
Thus, the time complexity is O(N^2).

### Example 2

```cpp
for(int i=0; i<N; i++) {
    for (int j=0; j<=i; j++) {
        // Block of code
    }
}
```

**Evaluate:**
In the outer for loop, `i` iterates `N` times. The inner loop iterates `i+1` times for each iteration of the outer loop. The total number of iterations is the sum of the first `N` integers, which is `N * (N + 1) / 2`. Therefore, the time complexity is O(N^2).

## Additional Information

### Importance of Time Complexity

Understanding time complexity helps in:

- **Algorithm Comparison:** Comparing different algorithms for efficiency.
- **Optimization:** Improving the performance of code.
- **Scalability:** Ensuring the algorithm performs well as the input size grows.

### Space Complexity

In addition to time complexity, space complexity is also important. It measures the amount of memory an algorithm uses relative to the input size.

### Common Time Complexities

- **O(1):** Constant time - The algorithm's performance is independent of the input size.
- **O(log N):** Logarithmic time - The algorithm's performance grows logarithmically.
- **O(N):** Linear time - The algorithm's performance grows linearly.
- **O(N log N):** Linearithmic time - Common in efficient sorting algorithms.
- **O(N^2):** Quadratic time - Common in algorithms with nested loops.
- **O(2^N):** Exponential time - Algorithms with recursive calls that double with each addition to the input size.
